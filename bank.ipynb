{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6a255f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install scikit-learn for all the core metrics and modeling\n",
    "# %pip install scikit-learn\n",
    "\n",
    "# # Install imbalanced-learn for handling imbalanced datasets (SMOTE)\n",
    "# %pip install imbalanced-learn\n",
    "\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install -q scikit-learn imbalanced-learn joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d86acb2",
   "metadata": {},
   "source": [
    "# 1: Project Setup and Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858b60bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded.\n",
      "INITIAL DATA OVERVIEW\n",
      "\n",
      "First 5 rows:\n",
      "   age          job  marital          education default  housing     loan  \\\n",
      "0   30  blue-collar  married           basic.9y      no      yes       no   \n",
      "1   39     services   single        high.school      no       no       no   \n",
      "2   25     services  married        high.school      no      yes       no   \n",
      "3   38     services  married           basic.9y      no  unknown  unknown   \n",
      "4   47       admin.  married  university.degree      no      yes       no   \n",
      "\n",
      "     contact month day_of_week  ...  campaign  pdays  previous     poutcome  \\\n",
      "0   cellular   may         fri  ...         2    999         0  nonexistent   \n",
      "1  telephone   may         fri  ...         4    999         0  nonexistent   \n",
      "2  telephone   jun         wed  ...         1    999         0  nonexistent   \n",
      "3  telephone   jun         fri  ...         3    999         0  nonexistent   \n",
      "4   cellular   nov         mon  ...         1    999         0  nonexistent   \n",
      "\n",
      "  emp.var.rate  cons.price.idx  cons.conf.idx  euribor3m  nr.employed   y  \n",
      "0         -1.8          92.893          -46.2      1.313       5099.1  no  \n",
      "1          1.1          93.994          -36.4      4.855       5191.0  no  \n",
      "2          1.4          94.465          -41.8      4.962       5228.1  no  \n",
      "3          1.4          94.465          -41.8      4.959       5228.1  no  \n",
      "4         -0.1          93.200          -42.0      4.191       5195.8  no  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "\n",
      "= BASIC STATISTICS =\n",
      "                age     job  marital          education default housing  loan  \\\n",
      "count   4119.000000    4119     4119               4119    4119    4119  4119   \n",
      "unique          NaN      12        4                  8       3       3     3   \n",
      "top             NaN  admin.  married  university.degree      no     yes    no   \n",
      "freq            NaN    1012     2509               1264    3315    2175  3349   \n",
      "mean      40.113620     NaN      NaN                NaN     NaN     NaN   NaN   \n",
      "std       10.313362     NaN      NaN                NaN     NaN     NaN   NaN   \n",
      "min       18.000000     NaN      NaN                NaN     NaN     NaN   NaN   \n",
      "25%       32.000000     NaN      NaN                NaN     NaN     NaN   NaN   \n",
      "50%       38.000000     NaN      NaN                NaN     NaN     NaN   NaN   \n",
      "75%       47.000000     NaN      NaN                NaN     NaN     NaN   NaN   \n",
      "max       88.000000     NaN      NaN                NaN     NaN     NaN   NaN   \n",
      "\n",
      "         contact month day_of_week  ...     campaign        pdays  \\\n",
      "count       4119  4119        4119  ...  4119.000000  4119.000000   \n",
      "unique         2    10           5  ...          NaN          NaN   \n",
      "top     cellular   may         thu  ...          NaN          NaN   \n",
      "freq        2652  1378         860  ...          NaN          NaN   \n",
      "mean         NaN   NaN         NaN  ...     2.537266   960.422190   \n",
      "std          NaN   NaN         NaN  ...     2.568159   191.922786   \n",
      "min          NaN   NaN         NaN  ...     1.000000     0.000000   \n",
      "25%          NaN   NaN         NaN  ...     1.000000   999.000000   \n",
      "50%          NaN   NaN         NaN  ...     2.000000   999.000000   \n",
      "75%          NaN   NaN         NaN  ...     3.000000   999.000000   \n",
      "max          NaN   NaN         NaN  ...    35.000000   999.000000   \n",
      "\n",
      "           previous     poutcome emp.var.rate  cons.price.idx  cons.conf.idx  \\\n",
      "count   4119.000000         4119  4119.000000     4119.000000    4119.000000   \n",
      "unique          NaN            3          NaN             NaN            NaN   \n",
      "top             NaN  nonexistent          NaN             NaN            NaN   \n",
      "freq            NaN         3523          NaN             NaN            NaN   \n",
      "mean       0.190337          NaN     0.084972       93.579704     -40.499102   \n",
      "std        0.541788          NaN     1.563114        0.579349       4.594578   \n",
      "min        0.000000          NaN    -3.400000       92.201000     -50.800000   \n",
      "25%        0.000000          NaN    -1.800000       93.075000     -42.700000   \n",
      "50%        0.000000          NaN     1.100000       93.749000     -41.800000   \n",
      "75%        0.000000          NaN     1.400000       93.994000     -36.400000   \n",
      "max        6.000000          NaN     1.400000       94.767000     -26.900000   \n",
      "\n",
      "          euribor3m  nr.employed     y  \n",
      "count   4119.000000  4119.000000  4119  \n",
      "unique          NaN          NaN     2  \n",
      "top             NaN          NaN    no  \n",
      "freq            NaN          NaN  3668  \n",
      "mean       3.621356  5166.481695   NaN  \n",
      "std        1.733591    73.667904   NaN  \n",
      "min        0.635000  4963.600000   NaN  \n",
      "25%        1.334000  5099.100000   NaN  \n",
      "50%        4.857000  5191.000000   NaN  \n",
      "75%        4.961000  5228.100000   NaN  \n",
      "max        5.045000  5228.100000   NaN  \n",
      "\n",
      "[11 rows x 21 columns]\n",
      "\n",
      "Data Types and Missing Values:\n",
      "✓ No missing values found\n",
      "\n",
      "== DUPLICATES ==\n",
      "Found 0 duplicate rows\n",
      "\n",
      "Statistical Summary:\n",
      "                age     job  marital          education default housing  loan  \\\n",
      "count   4119.000000    4119     4119               4119    4119    4119  4119   \n",
      "unique          NaN      12        4                  8       3       3     3   \n",
      "top             NaN  admin.  married  university.degree      no     yes    no   \n",
      "freq            NaN    1012     2509               1264    3315    2175  3349   \n",
      "mean      40.113620     NaN      NaN                NaN     NaN     NaN   NaN   \n",
      "std       10.313362     NaN      NaN                NaN     NaN     NaN   NaN   \n",
      "min       18.000000     NaN      NaN                NaN     NaN     NaN   NaN   \n",
      "25%       32.000000     NaN      NaN                NaN     NaN     NaN   NaN   \n",
      "50%       38.000000     NaN      NaN                NaN     NaN     NaN   NaN   \n",
      "75%       47.000000     NaN      NaN                NaN     NaN     NaN   NaN   \n",
      "max       88.000000     NaN      NaN                NaN     NaN     NaN   NaN   \n",
      "\n",
      "         contact month day_of_week  ...     campaign        pdays  \\\n",
      "count       4119  4119        4119  ...  4119.000000  4119.000000   \n",
      "unique         2    10           5  ...          NaN          NaN   \n",
      "top     cellular   may         thu  ...          NaN          NaN   \n",
      "freq        2652  1378         860  ...          NaN          NaN   \n",
      "mean         NaN   NaN         NaN  ...     2.537266   960.422190   \n",
      "std          NaN   NaN         NaN  ...     2.568159   191.922786   \n",
      "min          NaN   NaN         NaN  ...     1.000000     0.000000   \n",
      "25%          NaN   NaN         NaN  ...     1.000000   999.000000   \n",
      "50%          NaN   NaN         NaN  ...     2.000000   999.000000   \n",
      "75%          NaN   NaN         NaN  ...     3.000000   999.000000   \n",
      "max          NaN   NaN         NaN  ...    35.000000   999.000000   \n",
      "\n",
      "           previous     poutcome emp.var.rate  cons.price.idx  cons.conf.idx  \\\n",
      "count   4119.000000         4119  4119.000000     4119.000000    4119.000000   \n",
      "unique          NaN            3          NaN             NaN            NaN   \n",
      "top             NaN  nonexistent          NaN             NaN            NaN   \n",
      "freq            NaN         3523          NaN             NaN            NaN   \n",
      "mean       0.190337          NaN     0.084972       93.579704     -40.499102   \n",
      "std        0.541788          NaN     1.563114        0.579349       4.594578   \n",
      "min        0.000000          NaN    -3.400000       92.201000     -50.800000   \n",
      "25%        0.000000          NaN    -1.800000       93.075000     -42.700000   \n",
      "50%        0.000000          NaN     1.100000       93.749000     -41.800000   \n",
      "75%        0.000000          NaN     1.400000       93.994000     -36.400000   \n",
      "max        6.000000          NaN     1.400000       94.767000     -26.900000   \n",
      "\n",
      "          euribor3m  nr.employed     y  \n",
      "count   4119.000000  4119.000000  4119  \n",
      "unique          NaN          NaN     2  \n",
      "top             NaN          NaN    no  \n",
      "freq            NaN          NaN  3668  \n",
      "mean       3.621356  5166.481695   NaN  \n",
      "std        1.733591    73.667904   NaN  \n",
      "min        0.635000  4963.600000   NaN  \n",
      "25%        1.334000  5099.100000   NaN  \n",
      "50%        4.857000  5191.000000   NaN  \n",
      "75%        4.961000  5228.100000   NaN  \n",
      "max        5.045000  5228.100000   NaN  \n",
      "\n",
      "[11 rows x 21 columns]\n",
      "TARGET VARIABLE DISTRIBUTION\n",
      "y\n",
      "no     3668\n",
      "yes     451\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class Imbalance Ratio: 8.13:1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, roc_auc_score, confusion_matrix, \n",
    "                             classification_report, roc_curve)\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline \n",
    "\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv('bank-additional.csv', sep=';')\n",
    "    print(\"Data loaded.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: bank-additional.csv not found. Please ensure the file is in the correct path and try again.\")\n",
    "    df = pd.DataFrame() \n",
    "\n",
    "if not df.empty:\n",
    "\n",
    "    print(\"INITIAL DATA OVERVIEW\")\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df.head())\n",
    "\n",
    "    print(\"\\n= BASIC STATISTICS =\")\n",
    "    print(df.describe(include='all'))\n",
    "\n",
    "    print(\"\\nData Types and Missing Values:\")\n",
    "    missing = df.isnull().sum()\n",
    "    if missing.sum() == 0:\n",
    "        print(\"✓ No missing values found\")\n",
    "    else:\n",
    "        print(missing[missing > 0])\n",
    "\n",
    "    # Check duplicates\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"\\n== DUPLICATES ==\")\n",
    "    print(f\"Found {duplicates} duplicate rows\")\n",
    "    \n",
    "    print(\"\\nStatistical Summary:\")\n",
    "    print(df.describe(include='all'))\n",
    "    \n",
    "    # Target variable distribution\n",
    "    print(\"TARGET VARIABLE DISTRIBUTION\")\n",
    "    print(df['y'].value_counts())\n",
    "    print(f\"\\nClass Imbalance Ratio: {df['y'].value_counts()['no']/df['y'].value_counts()['yes']:.2f}:1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc7730c",
   "metadata": {},
   "source": [
    "# 2: Data Cleaning and Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1932435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Target variable 'y' successfully encoded (yes=1, no=0).\n",
      "\n",
      "'Unknown' values in categorical features:\n",
      "  • job: 39 (0.9%)\n",
      "  • marital: 11 (0.3%)\n",
      "  • education: 167 (4.1%)\n",
      "  • default: 803 (19.5%)\n",
      "  • housing: 105 (2.5%)\n",
      "  • loan: 105 (2.5%)\n",
      " No duplicate rows found.\n",
      "\n",
      "✓ 3 New features created: 'was_previously_contacted', 'campaign_successful', 'poutcome_success'.\n",
      "\n",
      "Final Target Distribution:\n",
      "y\n",
      "0    0.890507\n",
      "1    0.109493\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if not df.empty:\n",
    "\n",
    "    # 1. Data Cleaning: Target Variable Encoding \n",
    "    # Convert 'yes'/'no' to 1/0 immediately after loading\n",
    "    df['y'] = df['y'].map({'yes': 1, 'no': 0})\n",
    "    print(\"✓ Target variable 'y' successfully encoded (yes=1, no=0).\")\n",
    "    \n",
    "    # Check for NaN values in the target *after* encoding\n",
    "    if df['y'].isnull().any():\n",
    "        df.dropna(subset=['y'], inplace=True)\n",
    "        print(f\" Removed {df['y'].isnull().sum()} rows with NaN in target.\")\n",
    "\n",
    "    # 2. Check for 'unknown' values and duplicates\n",
    "    print(\"\\n'Unknown' values in categorical features:\")\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        unknown_count = (df[col] == 'unknown').sum()\n",
    "        if unknown_count > 0:\n",
    "            print(f\"  • {col}: {unknown_count} ({unknown_count/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    duplicates = df.duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        print(f\" Removed {duplicates} duplicate rows.\")\n",
    "    else:\n",
    "        print(\" No duplicate rows found.\")\n",
    "\n",
    "    # **Feature 1: Has been contacted before?** (0 if pdays=999, 1 otherwise)\n",
    "    df['was_previously_contacted'] = df['pdays'].apply(lambda x: 0 if x == 999 else 1)\n",
    "    \n",
    "    # **Feature 2: Campaign Efficiency Indicator** (1 if less than 5 contacts, 0 otherwise)\n",
    "    df['campaign_successful'] = df['campaign'].apply(lambda x: 1 if x < 5 else 0)\n",
    "    \n",
    "    # **Feature 3: Simplify 'poutcome'** (1 for success, 0 for failure/nonexistent)\n",
    "    df['poutcome_success'] = df['poutcome'].map({'success': 1, 'failure': 0, 'nonexistent': 0})\n",
    "    print(\"\\n✓ 3 New features created: 'was_previously_contacted', 'campaign_successful', 'poutcome_success'.\")\n",
    "\n",
    "    # Final check of the target distribution after cleaning/encoding\n",
    "    print(\"\\nFinal Target Distribution:\")\n",
    "    print(df['y'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca271441",
   "metadata": {},
   "source": [
    "# 4: Data Preprocessing and Creating Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cf5af58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 3295 samples\n",
      "TestingF set size: 824 samples\n",
      "\n",
      "ColumnTransformer (Preprocessor) created for scaling and encoding.\n",
      "\n",
      "Training set size after SMOTE: 5868 samples (Balanced)\n",
      "Target distribution after SMOTE: \n",
      "y\n",
      "1    2934\n",
      "0    2934\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if not df.empty:\n",
    "    # Separate features (X) and target (y)\n",
    "    X = df.drop('y', axis=1)\n",
    "    y = df['y']\n",
    "\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "    print(f\"TestingF set size: {X_test.shape[0]} samples\")\n",
    "\n",
    "    # Define feature types\n",
    "    numerical_features = ['age', 'campaign', 'previous', 'emp.var.rate', \n",
    "                          'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed',\n",
    "                          'was_previously_contacted', 'campaign_efficient', 'poutcome_success']\n",
    "    \n",
    "    categorical_features = ['job', 'marital', 'education', 'default', 'housing', \n",
    "                           'loan', 'contact', 'month', 'day_of_week', 'poutcome']\n",
    "    \n",
    "    # Features to scale (excluding binary engineered features)\n",
    "    features_to_scale = ['age', 'campaign', 'previous', 'emp.var.rate', \n",
    "                         'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed']\n",
    "    \n",
    "\n",
    "    # 2. Engineered/Binary Features (To be passed through without scaling)\n",
    "    engineered_features = ['was_previously_contacted', 'campaign_successful', 'poutcome_success']\n",
    "    \n",
    "     # 'duration' is removed for realistic modeling, 'pdays' is replaced by 'was_previously_contacted'\n",
    "    # Check for features to remove from the scaling list \n",
    "    for col in engineered_features + ['pdays', 'duration']:\n",
    "        if col in features_to_scale:\n",
    "             features_to_scale.remove(col)\n",
    "\n",
    "\n",
    "    # Create a preprocessor using ColumnTransformer\n",
    "        preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), features_to_scale),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features),\n",
    "            ('bin', 'passthrough', engineered_features)\n",
    "        ],\n",
    "        # CRITICAL FIX: Drop 'pdays' and 'duration'. Only explicitly listed features are kept.\n",
    "        remainder='drop' \n",
    "    )\n",
    "    print(\"\\nColumnTransformer (Preprocessor) created for scaling and encoding.\")\n",
    "\n",
    "# --- SMOTE Application (for reporting, showing the effect of balancing) ---\n",
    "    \n",
    "    # Apply preprocessor fit/transform once\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    \n",
    "    # Apply SMOTE to address class imbalance \n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_smote, y_train_smote = smote.fit_resample(X_train_processed, y_train)\n",
    "\n",
    "    print(f\"\\nTraining set size after SMOTE: {X_train_smote.shape[0]} samples (Balanced)\")\n",
    "    print(f\"Target distribution after SMOTE: \\n{y_train_smote.value_counts()}\")\n",
    "    \n",
    "    # NOTE: The final modeling (Cells 55 & 56) will use the base X_train/y_train \n",
    "    # and rely on the `class_weight='balanced'` parameter for robust cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f00ab8",
   "metadata": {},
   "source": [
    "# 5: Feature Selection and Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad26942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and Evaluating Models (using binary labels and class_weight='balanced')...\n",
      "--- Logistic Regression ---\n",
      "F1-Score: 0.6000, Recall: 0.8333, ROC AUC: 0.9423\n",
      "--- Random Forest ---\n",
      "F1-Score: 0.3871, Recall: 0.2667, ROC AUC: 0.9422\n",
      "--- MLP Classifier ---\n",
      "F1-Score: 0.6087, Recall: 0.7000, ROC AUC: 0.9333\n",
      "\n",
      "Selected Model for Tuning (based on F1-Score): **MLP Classifier**\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if not df.empty:\n",
    "\n",
    "    # Re-define fixed preprocessor for robustness within this cell\n",
    "    features_to_scale = ['age', 'campaign', 'previous', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed']\n",
    "    categorical_features = X_train.select_dtypes(include='object').columns.tolist()\n",
    "    engineered_passthrough = ['was_previously_contacted', 'campaign_successful', 'poutcome_success']\n",
    "    \n",
    "    preprocessor_fixed = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), features_to_scale),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features),\n",
    "            ('bin', 'passthrough', engineered_passthrough)\n",
    "        ],\n",
    "        remainder='drop' \n",
    "    )\n",
    " \n",
    "    # --- Define Pipelines for Model Comparison (Using class_weight='balanced' for imbalance) ---\n",
    "    \n",
    "    logreg_pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                  ('classifier', LogisticRegression(random_state=42, solver='liblinear', class_weight='balanced'))])\n",
    "\n",
    "    rf_pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('classifier', RandomForestClassifier(random_state=42, class_weight='balanced'))])\n",
    "\n",
    "    mlp_pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('classifier', MLPClassifier(random_state=42, max_iter=300))])\n",
    "\n",
    "    models = {'Logistic Regression': logreg_pipe, 'Random Forest': rf_pipe, 'MLP Classifier': mlp_pipe}\n",
    "\n",
    "    results = {}\n",
    "    print(\"\\nTraining and Evaluating Models (using binary labels and class_weight='balanced')...\")\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train) \n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Metrics now calculated correctly on binary labels\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "        \n",
    "        results[name] = {'F1-Score': f1, 'Recall': recall, 'ROC AUC': roc_auc}\n",
    "        print(f\"--- {name} ---\")\n",
    "        print(f\"F1-Score: {f1:.4f}, Recall: {recall:.4f}, ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "    best_model_name = max(results, key=lambda name: results[name]['F1-Score'])\n",
    "    best_model_pipe = models[best_model_name]\n",
    "    print(f\"\\nSelected Model for Tuning (based on F1-Score): **{best_model_name}**\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0941319",
   "metadata": {},
   "source": [
    "# 6: Hyperparameter Tuning and Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7f707a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting GridSearchCV for MLP Classifier with Feature Selection...\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "\n",
      "Best Parameters found by GridSearchCV:\n",
      "{'classifier__alpha': 0.0001, 'classifier__hidden_layer_sizes': (50,), 'classifier__learning_rate_init': 0.001, 'feature_selection__k': 'all'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "from sklearn.neural_network import MLPClassifier # Ensure MLP is imported\n",
    "\n",
    "if not df.empty:\n",
    "   \n",
    "    best_model_name = 'MLP Classifier' \n",
    "    \n",
    "    # 1. Re-define the preprocessor \n",
    "    feature_selector = SelectKBest(score_func=f_classif)\n",
    "\n",
    "\n",
    "    # 2. Define the FINAL PIPELINE: Preprocessor -> Feature Selector -> Classifier\n",
    "    final_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('feature_selection', feature_selector), \n",
    "        ('classifier', MLPClassifier(random_state=42, max_iter=500)) # Increased max_iter for tuning stability\n",
    "    ])\n",
    "    \n",
    "    # 3. Define the parameter grid, including parameters for the new steps\n",
    "    param_grid = {\n",
    "        # Tuning parameters for Feature Selection (Selecting top K features)\n",
    "        'feature_selection__k': [20, 30, 'all'], # Tune the number of features to keep\n",
    "        \n",
    "        # Tuning parameters for the Classifier\n",
    "        'classifier__hidden_layer_sizes': [(50,), (100, 50,), (100,)],\n",
    "        'classifier__alpha': [0.0001, 0.001], # L2 regularization term\n",
    "        'classifier__learning_rate_init': [0.001, 0.01]\n",
    "        # MLP does not natively support class_weight, so we rely on tuning parameters\n",
    "    }\n",
    "\n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        final_pipeline, \n",
    "        param_grid, \n",
    "        cv=5, \n",
    "        scoring='roc_auc', # Optimize for ROC AUC\n",
    "        n_jobs=-1, \n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(f\"\\nStarting GridSearchCV for {best_model_name} with Feature Selection...\")\n",
    " \n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    final_model = grid_search.best_estimator_\n",
    "    print(\"\\nBest Parameters found by GridSearchCV:\")\n",
    "    print(grid_search.best_params_)\n",
    "\n",
    "# if not df.empty:\n",
    "\n",
    "\n",
    "#     # Define parameter grid based on best model\n",
    "#     if best_model_name == 'Random Forest':\n",
    "#         param_grid = {\n",
    "#             'classifier__n_estimators': [100, 200],\n",
    "#             'classifier__max_depth': [10, 20, None],\n",
    "#             'classifier__min_samples_split': [2, 5],\n",
    "#             'classifier__class_weight': ['balanced', 'balanced_subsample']\n",
    "#         }\n",
    "#         base_model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "#     elif best_model_name == 'Logistic Regression':\n",
    "#         param_grid = {\n",
    "#             'classifier__C': [0.1, 1, 10],\n",
    "#             'classifier__penalty': ['l1', 'l2'],\n",
    "#             'classifier__class_weight': ['balanced', None]\n",
    "#         }\n",
    "#         base_model = LogisticRegression(random_state=42, solver='liblinear', max_iter=1000)\n",
    "#     else:  # Neural Network\n",
    "#         param_grid = {\n",
    "#             'classifier__hidden_layer_sizes': [(100,), (100, 50), (50, 50)],\n",
    "#             'classifier__alpha': [0.0001, 0.001],\n",
    "#             'classifier__learning_rate': ['constant', 'adaptive']\n",
    "#         }\n",
    "#         base_model = MLPClassifier(random_state=42, max_iter=500, early_stopping=True)\n",
    "    \n",
    "#     # Create pipeline with feature selection\n",
    "#     final_pipeline = Pipeline([\n",
    "#         ('preprocessor', preprocessor),\n",
    "#         ('feature_selection', SelectFromModel(\n",
    "#             RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "#             threshold='median'\n",
    "#         )),\n",
    "#         ('classifier', base_model)\n",
    "#     ])\n",
    "    \n",
    "#     # Grid search\n",
    "#     grid_search = GridSearchCV(\n",
    "#         final_pipeline,\n",
    "#         param_grid,\n",
    "#         cv=5,\n",
    "#         scoring='f1',\n",
    "#         n_jobs=-1,\n",
    "#         verbose=1\n",
    "#     )\n",
    "    \n",
    "#     print(\"Starting Grid Search...\")\n",
    "#     grid_search.fit(X_train, y_train)\n",
    "    \n",
    "#     print(\"\\n✓ Grid Search completed\")\n",
    "#     print(f\"\\nBest Parameters: {grid_search.best_params_}\")\n",
    "#     print(f\"Best CV F1-Score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "#     # Get best model\n",
    "#     final_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42db814e",
   "metadata": {},
   "source": [
    "# 7: Evaluation of the Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351698a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project.ipynb (Cell for Section 7: Evaluation of the Final Model)\n",
    "\n",
    "if not df.empty:\n",
    "    y_test_pred = final_model.predict(X_test)\n",
    "    y_test_proba = final_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    final_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    final_recall = recall_score(y_test, y_test_pred)\n",
    "    final_precision = precision_score(y_test, y_test_pred)\n",
    "    final_f1 = f1_score(y_test, y_test_pred)\n",
    "    final_roc_auc = roc_auc_score(y_test, y_test_proba)\n",
    "\n",
    "    print(\"\\n*** Final Model Performance on Test Set ***\")\n",
    "    print(f\"Accuracy: {final_accuracy:.4f}\")\n",
    "    print(f\"Recall (Subscription 'yes'): {final_recall:.4f}\")\n",
    "    print(f\"Precision (Subscription 'yes'): {final_precision:.4f}\")\n",
    "    print(f\"F1-Score: {final_f1:.4f}\")\n",
    "    print(f\"ROC AUC: {final_roc_auc:.4f}\")\n",
    "\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "\n",
    "    # Confusion Matrix Plot \n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['No (0)', 'Yes (1)'], \n",
    "                yticklabels=['No (0)', 'Yes (1)'])\n",
    "    plt.title('Confusion Matrix - Final Model (MLP)')\n",
    "    plt.ylabel('Actual Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "    # Feature Selection Output \n",
    "    try:\n",
    "        selector = final_model.named_steps['feature_selection']\n",
    "        print(\"\\nFeatures Selected by SelectKBest:\")\n",
    "        print(f\"Number of features kept: {selector.k_}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nCould not display feature selection details. Error: {e}\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        selector = final_model.named_steps['feature_selection']\n",
    "        # If the feature names are available, print them\n",
    "        print(\"\\nFeatures Selected by SelectFromModel:\")\n",
    "    \n",
    "        print(f\"Number of features kept: {selector.transform(X_test).shape[1]}\")\n",
    "    except Exception:\n",
    "        pass # Ignore if feature selection step is not found in the final model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
